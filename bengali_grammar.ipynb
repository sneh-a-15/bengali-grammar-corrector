{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdLQ_mscz_UM",
        "outputId": "ab3a9313-f913-4f1c-d7f9-1e28c7fdd9cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Step 1: Scraping and Filtering Bengali Wikipedia Pages ---\n",
            "Fetching: https://bn.wikipedia.org/wiki/‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂\n",
            "  ‚Üí Current unique sentences collected: 285\n",
            "Fetching: https://bn.wikipedia.org/wiki/‡¶≠‡¶æ‡¶∞‡¶§\n",
            "  ‚Üí Current unique sentences collected: 399\n",
            "Fetching: https://bn.wikipedia.org/wiki/‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó\n",
            "  ‚Üí Current unique sentences collected: 624\n",
            "Fetching: https://bn.wikipedia.org/wiki/‡¶¢‡¶æ‡¶ï‡¶æ\n",
            "  ‚Üí Current unique sentences collected: 706\n",
            "Fetching: https://bn.wikipedia.org/wiki/‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ\n",
            "  ‚Üí Current unique sentences collected: 906\n",
            "Fetching: https://bn.wikipedia.org/wiki/‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶•_‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞\n",
            "  ‚Üí Current unique sentences collected: 1056\n",
            "Fetching: https://bn.wikipedia.org/wiki/‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ_‡¶≠‡¶æ‡¶∑‡¶æ\n",
            "  ‚Üí Current unique sentences collected: 1108\n",
            "Fetching: https://bn.wikipedia.org/wiki/‡¶∂‡ßá‡¶ñ_‡¶Æ‡ßÅ‡¶ú‡¶ø‡¶¨‡ßÅ‡¶∞_‡¶∞‡¶π‡¶Æ‡¶æ‡¶®\n",
            "  ‚Üí Current unique sentences collected: 1277\n",
            "Fetching: https://bn.wikipedia.org/wiki/‡¶ï‡¶Æ‡ßç‡¶™‡¶ø‡¶â‡¶ü‡¶æ‡¶∞\n",
            "  ‚Üí Current unique sentences collected: 1320\n",
            "Fetching: https://bn.wikipedia.org/wiki/‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®\n",
            "  ‚Üí Current unique sentences collected: 1378\n",
            "\n",
            "‚úì Final count of STRICTLY FILTERED (Y) sentences: 1378.\n",
            "\n",
            "‚úì Saved 1378 clean sentences to 'clean_bengali_sentences.csv'\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "\n",
        "# --- 1. CONFIGURATION & SETUP ---\n",
        "\n",
        "# Maximum word count for sentences. Sticking to 10 for simplicity.\n",
        "STRICT_MAX_WORDS = 10\n",
        "OUTPUT_FILENAME = 'clean_bengali_sentences.csv'\n",
        "\n",
        "urls = [\n",
        "    \"https://bn.wikipedia.org/wiki/‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂\",\n",
        "    \"https://bn.wikipedia.org/wiki/‡¶≠‡¶æ‡¶∞‡¶§\",\n",
        "    \"https://bn.wikipedia.org/wiki/‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó\",\n",
        "    \"https://bn.wikipedia.org/wiki/‡¶¢‡¶æ‡¶ï‡¶æ\",\n",
        "    \"https://bn.wikipedia.org/wiki/‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ\",\n",
        "    \"https://bn.wikipedia.org/wiki/‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶•_‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞\",\n",
        "    \"https://bn.wikipedia.org/wiki/‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ_‡¶≠‡¶æ‡¶∑‡¶æ\",\n",
        "    \"https://bn.wikipedia.org/wiki/‡¶∂‡ßá‡¶ñ_‡¶Æ‡ßÅ‡¶ú‡¶ø‡¶¨‡ßÅ‡¶∞_‡¶∞‡¶π‡¶Æ‡¶æ‡¶®\",\n",
        "    \"https://bn.wikipedia.org/wiki/‡¶ï‡¶Æ‡ßç‡¶™‡¶ø‡¶â‡¶ü‡¶æ‡¶∞\",\n",
        "    \"https://bn.wikipedia.org/wiki/‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®\"\n",
        "]\n",
        "\n",
        "sentences = []\n",
        "\n",
        "print(\"--- Step 1: Scraping and Filtering Bengali Wikipedia Pages ---\")\n",
        "\n",
        "for url in urls:\n",
        "    try:\n",
        "        print(f\"Fetching: {url}\")\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        response.raise_for_status() # Raise exception for bad status codes\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        content = soup.find('div', {'id': 'mw-content-text'})\n",
        "\n",
        "        if content:\n",
        "            paragraphs = content.find_all('p')\n",
        "\n",
        "            for p in paragraphs:\n",
        "                text = p.get_text()\n",
        "                text = re.sub(r'\\[\\d+\\]', '', text) # Remove citations\n",
        "                sents = re.split(r'[‡•§\\n]', text) # Split by Bengali period or newline\n",
        "\n",
        "                for s in sents:\n",
        "                    s = s.strip()\n",
        "                    s = re.sub(r'\\([^)]*\\)', '', s) # Remove parentheses content\n",
        "                    s = s.strip()\n",
        "\n",
        "                    words = s.split()\n",
        "\n",
        "                    # --- STRICT FILTERING ---\n",
        "                    # 1. 5 <= words <= STRICT_MAX_WORDS (Max 10 words)\n",
        "                    # 2. Must contain Bengali characters\n",
        "                    # 3. Exclude complex markup/non-standard chars\n",
        "                    if 5 <= len(words) <= STRICT_MAX_WORDS and \\\n",
        "                       re.search(r'[\\u0980-\\u09FF]', s) and \\\n",
        "                       '|' not in s and '{' not in s:\n",
        "\n",
        "                        # Add the Bengali period back for clean output (Y data)\n",
        "                        sentences.append(s + '‡•§')\n",
        "\n",
        "        print(f\"  ‚Üí Current unique sentences collected: {len(set(sentences))}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"  ‚úó Error fetching {url}: {e}\")\n",
        "        continue\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Unexpected error on {url}: {e}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "# --- 2. FINALIZATION AND SAVE ---\n",
        "\n",
        "# Ensure unique sentences and save the clean list\n",
        "sentences = list(set(sentences))\n",
        "print(f\"\\n‚úì Final count of STRICTLY FILTERED (Y) sentences: {len(sentences)}.\")\n",
        "\n",
        "# Save to CSV\n",
        "if len(sentences) > 0:\n",
        "    df = pd.DataFrame({'correct_sentence': sentences})\n",
        "    df.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8')\n",
        "    print(f\"\\n‚úì Saved {len(sentences)} clean sentences to '{OUTPUT_FILENAME}'\")\n",
        "else:\n",
        "    print(\"\\n‚úó No sentences collected. Cannot proceed to noise injection.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "552HR9dJ3IPa"
      },
      "source": [
        "# error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqxE-unK1xt9",
        "outputId": "ef33f994-32dd-40b4-d2e1-5074be8fb445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 1. Loading Clean Data from 'clean_bengali_sentences.csv' ---\n",
            "‚úì Loaded 1378 unique clean sentences.\n",
            "\n",
            "--- 2. Generating 50000 Synthetic Error Pairs ---\n",
            "\n",
            "‚úì Dataset ready! Saved 50000 paired samples to 'bengali_simple_gec_training_data.csv'\n",
            "\n",
            "--- Sample Synthetic Training Pairs (X, Y) ---\n",
            "ERROR (X): ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶π‡¶£ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶∞ ‡¶Ö‡¶™‡¶∞ ‡¶è‡¶ï ‡¶¨‡¶ø‡¶∂‡¶ø‡¶∑‡ßç‡¶ü ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ ‡¶π‡¶≤ ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶∏‡¶ø‡•§\n",
            "CORRECT (Y): ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶π‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶∞ ‡¶Ö‡¶™‡¶∞ ‡¶è‡¶ï ‡¶¨‡¶ø‡¶∂‡¶ø‡¶∑‡ßç‡¶ü ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ ‡¶π‡¶≤ ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶∏‡¶ø‡•§\n",
            "\n",
            "ERROR (X): ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶§‡¶•‡ßç‡¶Ø‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶è‡¶ï ‡¶Ö‡¶®‡ßç‡¶Ø‡¶§‡¶Æ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶ï‡ßá‡¶®‡ßç‡¶¶‡ßç‡¶∞‡•§\n",
            "CORRECT (Y): ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶§‡¶•‡ßç‡¶Ø‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ö‡¶®‡ßç‡¶Ø‡¶§‡¶Æ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶ï‡ßá‡¶®‡ßç‡¶¶‡ßç‡¶∞‡•§\n",
            "\n",
            "ERROR (X): ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶¨‡ßà‡¶ú‡ßç‡¶û‡¶æ‡¶£‡¶ø‡¶ï ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®‡ßá ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá ‡¶â‡ßé‡¶∏‡¶∞‡ßç‡¶ó ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡•§\n",
            "CORRECT (Y): ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶¨‡ßà‡¶ú‡ßç‡¶û‡¶æ‡¶®‡¶ø‡¶ï ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®‡ßá ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá ‡¶â‡ßé‡¶∏‡¶∞‡ßç‡¶ó ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡•§\n",
            "\n",
            "ERROR (X): ‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶£‡ßç‡¶¨‡¶®‡ßç‡¶§‡¶∞‡ßá ‡¶≤‡¶ï‡ßç‡¶∑‡¶æ‡¶ß‡¶ø‡¶ï ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡ßá‡¶∞ ‡¶Æ‡ßÉ‡¶§‡ßç‡¶Ø‡ßÅ ‡¶ò‡¶ü‡ßá‡•§\n",
            "CORRECT (Y): ‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶®‡ßç‡¶¨‡¶®‡ßç‡¶§‡¶∞‡ßá ‡¶≤‡¶ï‡ßç‡¶∑‡¶æ‡¶ß‡¶ø‡¶ï ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡ßá‡¶∞ ‡¶Æ‡ßÉ‡¶§‡ßç‡¶Ø‡ßÅ ‡¶ò‡¶ü‡ßá‡•§\n",
            "\n",
            "ERROR (X): ‡¶â‡ßé‡¶∏‡¶æ‡¶π ‡¶≠‡¶æ‡¶§‡¶æ ‡¶™‡ßç‡¶Ø‡¶æ‡¶ï‡ßá‡¶ú‡ßá ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßã‡¶®‡ßç‡¶®‡¶Ø‡¶º‡¶®‡ßá ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶§‡¶æ ‡¶ï‡¶∞‡¶æ‡•§\n",
            "CORRECT (Y): ‡¶â‡ßé‡¶∏‡¶æ‡¶π ‡¶≠‡¶æ‡¶§‡¶æ ‡¶™‡ßç‡¶Ø‡¶æ‡¶ï‡ßá‡¶ú‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßã‡¶®‡ßç‡¶®‡¶Ø‡¶º‡¶®‡ßá ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶§‡¶æ ‡¶ï‡¶∞‡¶æ‡•§\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "\n",
        "INPUT_FILENAME = 'clean_bengali_sentences.csv'\n",
        "OUTPUT_FILENAME = 'bengali_simple_gec_training_data.csv'\n",
        "TARGET_DATASET_SIZE = 50000\n",
        "\n",
        "# Defined confusion sets for Orthographic errors\n",
        "ORTHO_CONFUSION_MAP = {\n",
        "    '‡¶ø': '‡ßÄ', '‡ßÄ': '‡¶ø',    # Short/Long i\n",
        "    '‡¶®': '‡¶£', '‡¶£': '‡¶®',    # Dental/Retroflex n\n",
        "    '‡¶∏': '‡¶∂', '‡¶∂': '‡¶∏',    # Palatal/Dental s\n",
        "    '‡¶Ö': '‡¶Ü', '‡¶Ü': '‡¶Ö'    # Vowel slip\n",
        "}\n",
        "\n",
        "# Affixes/Particles we will target for corruption\n",
        "TARGET_AFFIXES = ['‡¶ü‡¶ø', '‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶ï‡ßá', '‡¶∞', '‡¶§‡ßá']\n",
        "AFFIX_SUBSTITUTION_MAP = {\n",
        "    '‡¶ï‡ßá': '‡¶∞',  # Swap object marker for possessive marker\n",
        "    '‡¶ü‡¶ø': '‡¶ó‡ßÅ‡¶≤‡ßã', # Swap singular particle for plural\n",
        "    '‡¶ó‡ßÅ‡¶≤‡ßã': '‡¶ü‡¶ø'  # Swap plural particle for singular\n",
        "}\n",
        "\n",
        "# --- 2. NOISE INJECTION FUNCTIONS ---\n",
        "\n",
        "def inject_ortho_error(correct_sentence):\n",
        "    \"\"\"Applies a single, focused orthographic substitution error.\"\"\"\n",
        "    chars = list(correct_sentence)\n",
        "\n",
        "    # 90% chance to attempt injecting an orthographic error\n",
        "    if random.random() < 0.9:\n",
        "        possible_indices = [i for i, char in enumerate(chars) if char in ORTHO_CONFUSION_MAP]\n",
        "\n",
        "        if possible_indices:\n",
        "            idx = random.choice(possible_indices)\n",
        "            original_char = chars[idx]\n",
        "\n",
        "            # Substitute the character with its confused counterpart\n",
        "            chars[idx] = ORTHO_CONFUSION_MAP.get(original_char, original_char)\n",
        "\n",
        "            error_text = \"\".join(chars)\n",
        "            # Only return the error if a change actually occurred\n",
        "            if error_text != correct_sentence:\n",
        "                return error_text\n",
        "\n",
        "    return None\n",
        "\n",
        "def inject_affix_error(correct_sentence):\n",
        "    \"\"\"Introduces an error by substituting or deleting a simple nominal affix/particle.\"\"\"\n",
        "\n",
        "    for original_affix in TARGET_AFFIXES:\n",
        "        if original_affix in correct_sentence:\n",
        "            # 1. Affix Deletion (Omission) - 50% chance\n",
        "            if random.random() < 0.5:\n",
        "                # Remove the first occurrence of the affix\n",
        "                error_text = correct_sentence.replace(original_affix, '', 1)\n",
        "                if error_text != correct_sentence:\n",
        "                    return error_text\n",
        "\n",
        "            # 2. Affix Substitution - 50% chance\n",
        "            if original_affix in AFFIX_SUBSTITUTION_MAP and random.random() < 0.5:\n",
        "                # Substitute the affix with a grammatically incorrect one\n",
        "                incorrect_affix = AFFIX_SUBSTITUTION_MAP[original_affix]\n",
        "                error_text = correct_sentence.replace(original_affix, incorrect_affix, 1)\n",
        "                if error_text != correct_sentence:\n",
        "                    return error_text\n",
        "\n",
        "    return None\n",
        "\n",
        "# --- 3. DATASET GENERATION EXECUTION ---\n",
        "\n",
        "def run_noise_injection_pipeline():\n",
        "    \"\"\"Loads clean data, generates noisy pairs, and saves the final training set.\"\"\"\n",
        "    if not os.path.exists(INPUT_FILENAME):\n",
        "        print(f\"‚úó Error: Input file '{INPUT_FILENAME}' not found. Run the collection script first.\")\n",
        "        return\n",
        "\n",
        "    print(f\"--- 1. Loading Clean Data from '{INPUT_FILENAME}' ---\")\n",
        "    df_clean = pd.read_csv(INPUT_FILENAME, encoding='utf-8')\n",
        "    correct_sentences = df_clean['correct_sentence'].tolist()\n",
        "\n",
        "    if not correct_sentences:\n",
        "        print(\"‚úó Error: Clean sentence list is empty. Cannot generate errors.\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚úì Loaded {len(correct_sentences)} unique clean sentences.\")\n",
        "\n",
        "    # 2. Generate (X, Y) pairs\n",
        "    print(f\"\\n--- 2. Generating {TARGET_DATASET_SIZE} Synthetic Error Pairs ---\")\n",
        "    final_dataset = []\n",
        "\n",
        "    # Loop continuously until the target size is met\n",
        "    while len(final_dataset) < TARGET_DATASET_SIZE:\n",
        "        # Loop through the clean sentences to generate variants\n",
        "        for correct_sentence in correct_sentences:\n",
        "            if len(final_dataset) >= TARGET_DATASET_SIZE:\n",
        "                break\n",
        "\n",
        "            # 2.1. Orthography Error (X1)\n",
        "            error_ortho = inject_ortho_error(correct_sentence)\n",
        "            if error_ortho and error_ortho != correct_sentence:\n",
        "                final_dataset.append({'error_text': error_ortho, 'correct_text': correct_sentence})\n",
        "\n",
        "            # 2.2. Nominal Affix Error (X2)\n",
        "            error_affix = inject_affix_error(correct_sentence)\n",
        "            if error_affix and error_affix != correct_sentence:\n",
        "                final_dataset.append({'error_text': error_affix, 'correct_text': correct_sentence})\n",
        "\n",
        "            # 2.3. Add the correct sentence as a \"no correction needed\" example (20% of time)\n",
        "            if random.random() < 0.2:\n",
        "                final_dataset.append({'error_text': correct_sentence, 'correct_text': correct_sentence})\n",
        "\n",
        "\n",
        "    # 3. Final Save and Sample\n",
        "    final_df = pd.DataFrame(final_dataset)\n",
        "\n",
        "    # Truncate to the target size and shuffle\n",
        "    final_df = final_df.head(TARGET_DATASET_SIZE).sample(frac=1).reset_index(drop=True)\n",
        "    final_df.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8')\n",
        "\n",
        "    print(f\"\\n‚úì Dataset ready! Saved {len(final_df)} paired samples to '{OUTPUT_FILENAME}'\")\n",
        "\n",
        "    print(\"\\n--- Sample Synthetic Training Pairs (X, Y) ---\")\n",
        "    for index, row in final_df.head(5).iterrows():\n",
        "        print(f\"ERROR (X): {row['error_text']}\")\n",
        "        print(f\"CORRECT (Y): {row['correct_text']}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_noise_injection_pipeline()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFlYdm-z3M9J"
      },
      "source": [
        "# duplicate removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "jYTg_7Fr28cP",
        "outputId": "a068e3f9-ef9d-4f41-99e1-ed07026332f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload the 'simple_50k.csv' file:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b784f8e1-1cee-4e47-961d-1d94b2ff24fd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b784f8e1-1cee-4e47-961d-1d94b2ff24fd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3911378678.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Run this cell and select your 'simple_50k.csv' file from your local machine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please upload the 'simple_50k.csv' file:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Get the filename (it should be 'simple_50k.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     result = _output.eval_js(\n\u001b[0m\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[1;32m    174\u001b[0m             \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# --- 1. Upload the File ---\n",
        "# Run this cell and select your 'simple_50k.csv' file from your local machine.\n",
        "print(\"Please upload the 'simple_50k.csv' file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename (it should be 'simple_50k.csv')\n",
        "file_name = next(iter(uploaded))\n",
        "\n",
        "# --- 2. Load the Dataset ---\n",
        "try:\n",
        "    # Read the uploaded file into a DataFrame\n",
        "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "    print(f\"\\nSuccessfully loaded {len(df)} rows from '{file_name}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError loading the file: {e}\")\n",
        "    # Stop execution if loading fails\n",
        "    exit()\n",
        "\n",
        "# --- 3. Identify and Remove Duplicates ---\n",
        "# Calculate the number of duplicate rows before removal\n",
        "initial_rows = len(df)\n",
        "duplicate_rows = df.duplicated().sum()\n",
        "\n",
        "# Create a new DataFrame with only unique rows\n",
        "df_unique = df.drop_duplicates()\n",
        "unique_rows = len(df_unique)\n",
        "\n",
        "print(f\"\\n--- Duplication Summary ---\")\n",
        "print(f\"Total Rows (Original): {initial_rows}\")\n",
        "print(f\"Duplicate Rows Found: {duplicate_rows}\")\n",
        "print(f\"Unique Rows (After Removal): {unique_rows}\")\n",
        "print(f\"Percentage of duplicates removed: {duplicate_rows / initial_rows * 100:.2f}%\")\n",
        "\n",
        "# --- 4. Save the Unique Dataset ---\n",
        "output_file_name = \"simple_50k_unique.csv\"\n",
        "\n",
        "# Save the unique DataFrame to a new CSV file\n",
        "df_unique.to_csv(output_file_name, index=False)\n",
        "\n",
        "# --- 5. Download the New File ---\n",
        "# This will trigger a download prompt in your browser.\n",
        "files.download(output_file_name)\n",
        "\n",
        "print(f\"\\nSuccessfully saved the unique data to '{output_file_name}'.\")\n",
        "print(\"The file should now be downloading to your computer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEuPikUVw6K8"
      },
      "source": [
        "# removed same same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByTRr3Q6wqBk",
        "outputId": "819ef7ec-4000-4b0b-c593-409f0f89b7cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Original dataset size: 14525 rows\n",
            "                                          error_text  \\\n",
            "0  ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶π‡¶£ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶∞ ‡¶Ö‡¶™‡¶∞ ‡¶è‡¶ï ‡¶¨‡¶ø‡¶∂‡¶ø‡¶∑‡ßç‡¶ü ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ...   \n",
            "1  ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶§‡¶•‡ßç‡¶Ø‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶è‡¶ï ‡¶Ö‡¶®‡ßç‡¶Ø‡¶§...   \n",
            "2  ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶¨‡ßà‡¶ú‡ßç‡¶û‡¶æ‡¶£‡¶ø‡¶ï ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®‡ßá ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ‡¶ó‡ßÅ‡¶≤...   \n",
            "3  ‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶£‡ßç‡¶¨‡¶®‡ßç‡¶§‡¶∞‡ßá ‡¶≤‡¶ï‡ßç‡¶∑‡¶æ‡¶ß‡¶ø‡¶ï ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡ßá‡¶∞ ...   \n",
            "4  ‡¶â‡ßé‡¶∏‡¶æ‡¶π ‡¶≠‡¶æ‡¶§‡¶æ ‡¶™‡ßç‡¶Ø‡¶æ‡¶ï‡ßá‡¶ú‡ßá ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßã‡¶®‡ßç‡¶®‡¶Ø‡¶º‡¶®‡ßá ‡¶∏‡¶π‡¶æ‡¶Ø...   \n",
            "\n",
            "                                        correct_text  \n",
            "0  ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶π‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶∞ ‡¶Ö‡¶™‡¶∞ ‡¶è‡¶ï ‡¶¨‡¶ø‡¶∂‡¶ø‡¶∑‡ßç‡¶ü ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ...  \n",
            "1  ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶§‡¶•‡ßç‡¶Ø‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ö‡¶®‡ßç...  \n",
            "2  ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶¨‡ßà‡¶ú‡ßç‡¶û‡¶æ‡¶®‡¶ø‡¶ï ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®‡ßá ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ‡¶ó‡ßÅ‡¶≤...  \n",
            "3  ‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶®‡ßç‡¶¨‡¶®‡ßç‡¶§‡¶∞‡ßá ‡¶≤‡¶ï‡ßç‡¶∑‡¶æ‡¶ß‡¶ø‡¶ï ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡ßá‡¶∞ ...  \n",
            "4  ‡¶â‡ßé‡¶∏‡¶æ‡¶π ‡¶≠‡¶æ‡¶§‡¶æ ‡¶™‡ßç‡¶Ø‡¶æ‡¶ï‡ßá‡¶ú‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßã‡¶®‡ßç‡¶®‡¶Ø‡¶º‡¶®‡ßá ‡¶∏‡¶π‡¶æ...  \n",
            "\n",
            "‚úÖ Removed 1363 identical rows.\n",
            "üìâ Cleaned dataset size: 13162 rows\n",
            "\n",
            "üíæ Cleaned dataset saved to: /content/simple_50k_unique_1.csv\n",
            "\n",
            "üîç Sample cleaned rows:\n",
            "                                          error_text  \\\n",
            "0  ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶π‡¶£ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶∞ ‡¶Ö‡¶™‡¶∞ ‡¶è‡¶ï ‡¶¨‡¶ø‡¶∂‡¶ø‡¶∑‡ßç‡¶ü ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ...   \n",
            "1  ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶§‡¶•‡ßç‡¶Ø‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶è‡¶ï ‡¶Ö‡¶®‡ßç‡¶Ø‡¶§...   \n",
            "2  ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶¨‡ßà‡¶ú‡ßç‡¶û‡¶æ‡¶£‡¶ø‡¶ï ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®‡ßá ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ‡¶ó‡ßÅ‡¶≤...   \n",
            "3  ‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶£‡ßç‡¶¨‡¶®‡ßç‡¶§‡¶∞‡ßá ‡¶≤‡¶ï‡ßç‡¶∑‡¶æ‡¶ß‡¶ø‡¶ï ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡ßá‡¶∞ ...   \n",
            "4  ‡¶â‡ßé‡¶∏‡¶æ‡¶π ‡¶≠‡¶æ‡¶§‡¶æ ‡¶™‡ßç‡¶Ø‡¶æ‡¶ï‡ßá‡¶ú‡ßá ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßã‡¶®‡ßç‡¶®‡¶Ø‡¶º‡¶®‡ßá ‡¶∏‡¶π‡¶æ‡¶Ø...   \n",
            "\n",
            "                                        correct_text  \n",
            "0  ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶π‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶∞ ‡¶Ö‡¶™‡¶∞ ‡¶è‡¶ï ‡¶¨‡¶ø‡¶∂‡¶ø‡¶∑‡ßç‡¶ü ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ...  \n",
            "1  ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶§‡¶•‡ßç‡¶Ø‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ö‡¶®‡ßç...  \n",
            "2  ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶¨‡ßà‡¶ú‡ßç‡¶û‡¶æ‡¶®‡¶ø‡¶ï ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®‡ßá ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ‡¶ó‡ßÅ‡¶≤...  \n",
            "3  ‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶®‡ßç‡¶¨‡¶®‡ßç‡¶§‡¶∞‡ßá ‡¶≤‡¶ï‡ßç‡¶∑‡¶æ‡¶ß‡¶ø‡¶ï ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡ßá‡¶∞ ...  \n",
            "4  ‡¶â‡ßé‡¶∏‡¶æ‡¶π ‡¶≠‡¶æ‡¶§‡¶æ ‡¶™‡ßç‡¶Ø‡¶æ‡¶ï‡ßá‡¶ú‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßã‡¶®‡ßç‡¶®‡¶Ø‡¶º‡¶®‡ßá ‡¶∏‡¶π‡¶æ...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# üìÇ Step 1: Load your dataset\n",
        "# ============================================================\n",
        "# Replace with your actual CSV path\n",
        "CSV_PATH = \"/content/simple_50k_unique.csv\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH, encoding='utf-8')\n",
        "\n",
        "print(f\"üìä Original dataset size: {len(df)} rows\")\n",
        "print(df.head())\n",
        "\n",
        "# ============================================================\n",
        "# üßπ Step 2: Remove rows where error_text == correct_text\n",
        "# ============================================================\n",
        "before_count = len(df)\n",
        "\n",
        "# Drop rows where both columns are exactly same (after stripping spaces)\n",
        "df = df[df['error_text'].str.strip() != df['correct_text'].str.strip()]\n",
        "\n",
        "after_count = len(df)\n",
        "removed = before_count - after_count\n",
        "\n",
        "print(f\"\\n‚úÖ Removed {removed} identical rows.\")\n",
        "print(f\"üìâ Cleaned dataset size: {after_count} rows\")\n",
        "\n",
        "# ============================================================\n",
        "# üíæ Step 3: Save the cleaned dataset\n",
        "# ============================================================\n",
        "CLEAN_PATH = \"/content/simple_50k_unique_1.csv\"\n",
        "df.to_csv(CLEAN_PATH, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"\\nüíæ Cleaned dataset saved to: {CLEAN_PATH}\")\n",
        "\n",
        "# Show sample cleaned data\n",
        "print(\"\\nüîç Sample cleaned rows:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK44mprc3ws2"
      },
      "source": [
        "# final final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJRWln9h1re_",
        "outputId": "53d2ec45-2880-4bc8-dab8-beae8d988a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üáßüá© Improved Bengali Grammar Correction - Training\n",
            "============================================================\n",
            "üìä Train: 12504, Test: 658\n",
            "============================================================\n",
            "üìö Training Improved Bengali Grammar Corrector\n",
            "============================================================\n",
            "‚úì Loaded 12504 samples\n",
            "‚úì Built 12504 sentence mappings\n",
            "\n",
            "Extracting word-level and bigram rules ...\n",
            "‚úì 1655 reliable word corrections learned\n",
            "\n",
            "Analyzing edit patterns ...\n",
            "‚úì Found 15 frequent edit patterns\n",
            "\n",
            "‚úÖ Training completed!\n",
            "‚úì Model saved to: /content/model1/improved_corrector.pkl\n",
            "\n",
            "============================================================\n",
            "üìä Evaluating on Test Set\n",
            "============================================================\n",
            "Processed 300/100 sentences...\n",
            "Processed 200/100 sentences...\n",
            "‚úÖ Conservative Accuracy: 79.00%\n",
            "Processed 300/100 sentences...\n",
            "Processed 200/100 sentences...\n",
            "‚úÖ Aggressive Accuracy: 79.00%\n",
            "\n",
            "üèÅ Final Accuracy: 79.00%\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Improved Bengali Grammar Correction System (Optimized for Colab)\n",
        "Faster version with Levenshtein distance and progress updates\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import os\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ‚úÖ install and use fast edit distance\n",
        "try:\n",
        "    from Levenshtein import distance as fast_distance\n",
        "except ImportError:\n",
        "    print(\"Installing python-Levenshtein ...\")\n",
        "    import subprocess, sys\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"python-Levenshtein\"])\n",
        "    from Levenshtein import distance as fast_distance\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Improved Corrector with Better Logic\n",
        "# ============================================================\n",
        "\n",
        "class ImprovedBengaliCorrector:\n",
        "    \"\"\"\n",
        "    Improved corrector with:\n",
        "    1. Minimal edit distance (fast Levenshtein)\n",
        "    2. High-confidence corrections only\n",
        "    3. Word-level + bigram contextual rules\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.correction_dict = {}\n",
        "        self.word_corrections = defaultdict(lambda: defaultdict(int))\n",
        "        self.edit_patterns = []\n",
        "        self.best_word_corrections = {}\n",
        "        self.bigram_corrections = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # ============================================================\n",
        "    # TRAINING\n",
        "    # ============================================================\n",
        "    def train(self, csv_path, max_samples=14000):\n",
        "        print(\"=\" * 60)\n",
        "        print(\"üìö Training Improved Bengali Grammar Corrector\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
        "        df.columns = [\"incorrect\", \"correct\"]\n",
        "        if len(df) > max_samples:\n",
        "            df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
        "        print(f\"‚úì Loaded {len(df)} samples\")\n",
        "\n",
        "        # Sentence-level corrections\n",
        "        for _, row in df.iterrows():\n",
        "            self.correction_dict[row[\"incorrect\"].strip()] = row[\"correct\"].strip()\n",
        "        print(f\"‚úì Built {len(self.correction_dict)} sentence mappings\")\n",
        "\n",
        "        # Word & bigram-level rules\n",
        "        print(\"\\nExtracting word-level and bigram rules ...\")\n",
        "        for _, row in df.iterrows():\n",
        "            inc_words = str(row[\"incorrect\"]).split()\n",
        "            cor_words = str(row[\"correct\"]).split()\n",
        "            if len(inc_words) == len(cor_words):\n",
        "                for i, (inc, cor) in enumerate(zip(inc_words, cor_words)):\n",
        "                    if inc != cor:\n",
        "                        self.word_corrections[inc][cor] += 1\n",
        "                        if i > 0:\n",
        "                            prev = inc_words[i - 1]\n",
        "                            self.bigram_corrections[(prev, inc)][cor] += 1\n",
        "\n",
        "        # Best word corrections (‚â•2 occurrences)\n",
        "        for inc_word, variants in self.word_corrections.items():\n",
        "            cor, count = max(variants.items(), key=lambda x: x[1])\n",
        "            if count >= 2:\n",
        "                self.best_word_corrections[inc_word] = cor\n",
        "        print(f\"‚úì {len(self.best_word_corrections)} reliable word corrections learned\")\n",
        "\n",
        "        # Character-level edit patterns\n",
        "        print(\"\\nAnalyzing edit patterns ...\")\n",
        "        from collections import Counter\n",
        "        edits = Counter()\n",
        "        for inc, cor in self.correction_dict.items():\n",
        "            for e in self._get_edits(inc, cor):\n",
        "                edits[e] += 1\n",
        "        self.edit_patterns = [e for e, c in edits.items() if c >= 5]\n",
        "        print(f\"‚úì Found {len(self.edit_patterns)} frequent edit patterns\")\n",
        "\n",
        "        print(\"\\n‚úÖ Training completed!\")\n",
        "        return self\n",
        "\n",
        "    # ============================================================\n",
        "    # UTILITIES\n",
        "    # ============================================================\n",
        "    def _get_edits(self, s1, s2):\n",
        "        edits = []\n",
        "        matcher = SequenceMatcher(None, s1, s2)\n",
        "        for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
        "            if tag == \"replace\":\n",
        "                o, n = s1[i1:i2], s2[j1:j2]\n",
        "                if len(o) == len(n) == 1:\n",
        "                    edits.append((\"sub\", o, n))\n",
        "            elif tag == \"delete\":\n",
        "                edits.append((\"del\", s1[i1:i2], \"\"))\n",
        "            elif tag == \"insert\":\n",
        "                edits.append((\"ins\", \"\", s2[j1:j2]))\n",
        "        return edits\n",
        "\n",
        "    def _apply_edits(self, text):\n",
        "        corrected = text\n",
        "        for typ, old, new in self.edit_patterns:\n",
        "            if typ == \"sub\" and old in corrected:\n",
        "                corrected = corrected.replace(old, new)\n",
        "        return corrected\n",
        "\n",
        "    # ============================================================\n",
        "    # INFERENCE\n",
        "    # ============================================================\n",
        "    def correct(self, text, aggressive=False):\n",
        "        text = str(text).strip()\n",
        "        if not text:\n",
        "            return text\n",
        "\n",
        "        # 1Ô∏è‚É£ Exact match\n",
        "        if text in self.correction_dict:\n",
        "            return self.correction_dict[text]\n",
        "\n",
        "        # 2Ô∏è‚É£ Similar sentence (fast distance, limited search)\n",
        "        best_match, best_distance = None, float(\"inf\")\n",
        "        text_len = len(text)\n",
        "        for inc, cor in list(self.correction_dict.items())[:3000]:\n",
        "            if abs(len(inc) - text_len) > 4:\n",
        "                continue\n",
        "            d = fast_distance(text, inc)\n",
        "            if d <= 3 and d < best_distance:\n",
        "                best_distance, best_match = d, cor\n",
        "        if best_match and best_distance <= 2:\n",
        "            return best_match\n",
        "\n",
        "        # 3Ô∏è‚É£ Word-level correction\n",
        "        words, new_words = text.split(), []\n",
        "        changes = 0\n",
        "        for i, w in enumerate(words):\n",
        "            corrected = w\n",
        "            if i > 0 and (words[i - 1], w) in self.bigram_corrections:\n",
        "                opts = self.bigram_corrections[(words[i - 1], w)]\n",
        "                corrected = max(opts.items(), key=lambda x: x[1])[0]\n",
        "            elif w in self.best_word_corrections:\n",
        "                corrected = self.best_word_corrections[w]\n",
        "            if corrected != w:\n",
        "                changes += 1\n",
        "            new_words.append(corrected)\n",
        "        if changes > 0:\n",
        "            candidate = \" \".join(new_words)\n",
        "            if fast_distance(text, candidate) <= len(text) * 0.3:\n",
        "                return candidate\n",
        "\n",
        "        # 4Ô∏è‚É£ Aggressive mode (character-level)\n",
        "        if aggressive:\n",
        "            corrected = self._apply_edits(text)\n",
        "            if corrected != text:\n",
        "                return corrected\n",
        "\n",
        "        return text\n",
        "\n",
        "    # ============================================================\n",
        "    # SAVE / LOAD\n",
        "    # ============================================================\n",
        "    def save(self, model_dir=\"model\"):\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        model_path = os.path.join(model_dir, \"improved_corrector.pkl\")\n",
        "        with open(model_path, \"wb\") as f:\n",
        "            pickle.dump(\n",
        "                {\n",
        "                    \"correction_dict\": self.correction_dict,\n",
        "                    \"word_corrections\": dict(self.word_corrections),\n",
        "                    \"best_word_corrections\": self.best_word_corrections,\n",
        "                    \"edit_patterns\": self.edit_patterns,\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "        print(f\"‚úì Model saved to: {model_path}\")\n",
        "        return model_path\n",
        "\n",
        "    def load(self, model_path):\n",
        "        with open(model_path, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "        self.correction_dict = data[\"correction_dict\"]\n",
        "        self.word_corrections = defaultdict(lambda: defaultdict(int), data[\"word_corrections\"])\n",
        "        self.best_word_corrections = data[\"best_word_corrections\"]\n",
        "        self.edit_patterns = data[\"edit_patterns\"]\n",
        "        print(f\"‚úì Loaded model from: {model_path}\")\n",
        "        return self\n",
        "\n",
        "    # ============================================================\n",
        "    # EVALUATION\n",
        "    # ============================================================\n",
        "    def evaluate(self, test_df, aggressive=False):\n",
        "        correct = 0\n",
        "        total = len(test_df)\n",
        "        results = []\n",
        "\n",
        "        for idx, row in test_df.iterrows():\n",
        "            inc, exp = str(row[\"incorrect\"]).strip(), str(row[\"correct\"]).strip()\n",
        "            pred = self.correct(inc, aggressive=aggressive)\n",
        "            is_correct = pred == exp\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "            results.append(\n",
        "                {\"incorrect\": inc, \"expected\": exp, \"predicted\": pred, \"correct\": is_correct}\n",
        "            )\n",
        "\n",
        "            if (idx + 1) % 100 == 0:\n",
        "                print(f\"Processed {idx+1}/{total} sentences...\", flush=True)\n",
        "\n",
        "        acc = (correct / total) * 100 if total else 0\n",
        "        return acc, results\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "def train_improved_corrector():\n",
        "    CSV_PATH = \"/content/simple_50k_unique_1.csv\"\n",
        "    MODEL_DIR = \"/content/model1\"\n",
        "    MAX_SAMPLES = 14000\n",
        "\n",
        "    print(\"üáßüá© Improved Bengali Grammar Correction - Training\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    df = pd.read_csv(CSV_PATH, encoding=\"utf-8\")\n",
        "    df.columns = [\"incorrect\", \"correct\"]\n",
        "    if len(df) > MAX_SAMPLES:\n",
        "        df = df.sample(n=MAX_SAMPLES, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # 95/5 split\n",
        "    test_size = int(len(df) * 0.05)\n",
        "    test_df, train_df = df.iloc[:test_size], df.iloc[test_size:]\n",
        "    print(f\"üìä Train: {len(train_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "    train_csv = \"/content/train_improved.csv\"\n",
        "    train_df.to_csv(train_csv, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    corrector = ImprovedBengaliCorrector()\n",
        "    corrector.train(train_csv)\n",
        "    model_path = corrector.save(MODEL_DIR)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üìä Evaluating on Test Set\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # To avoid slowdown, test only on subset first\n",
        "    test_df = test_df.sample(min(100, len(test_df)), random_state=42)\n",
        "\n",
        "    acc_cons, res_cons = corrector.evaluate(test_df, aggressive=False)\n",
        "    print(f\"‚úÖ Conservative Accuracy: {acc_cons:.2f}%\")\n",
        "\n",
        "    acc_aggr, res_aggr = corrector.evaluate(test_df, aggressive=True)\n",
        "    print(f\"‚úÖ Aggressive Accuracy: {acc_aggr:.2f}%\")\n",
        "\n",
        "    final_acc = max(acc_cons, acc_aggr)\n",
        "    print(f\"\\nüèÅ Final Accuracy: {final_acc:.2f}%\")\n",
        "\n",
        "    return corrector, final_acc\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TESTING\n",
        "# ============================================================\n",
        "\n",
        "def test_improved():\n",
        "    MODEL_PATH = \"/content/model1/improved_corrector.pkl\"\n",
        "    corrector = ImprovedBengaliCorrector().load(MODEL_PATH)\n",
        "\n",
        "    tests = [\n",
        "        \"‡¶ñ‡ßç‡¶∞‡¶ø‡¶∑‡ßç‡¶ü‡¶ß‡¶∞‡ßç‡¶Æ ‡¶π‡¶≤ ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶ö‡¶§‡ßÅ‡¶∞‡ßç‡¶• ‡¶¨‡ßÉ‡¶π‡¶§‡ßç‡¶§‡¶Æ ‡¶ß‡¶∞‡ßç‡¶Æ, ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º ‡ß¶.‡ß™ ‡¶∏‡¶§‡¶æ‡¶Ç‡¶∂‡•§\",\n",
        "        \"‡¶≠‡ßå‡¶ó‡ßã‡¶≤‡¶ø‡¶ï ‡¶¨‡¶ø‡¶ö‡¶æ‡¶∞‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∏‡ßá‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶¶‡¶ï‡ßç‡¶∑‡¶ø‡¶£ ‡¶è‡¶∂‡¶ø‡¶Ø‡¶º‡¶æ‡¶Ø‡¶º‡•§\",\n",
        "        \"‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶£ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶∏‡ßÄ‡¶Æ‡¶æ‡¶®‡¶æ‡•§\",\n",
        "        \"‡¶§‡¶ø‡¶®‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶¨‡ßÄ‡¶ú‡ßç‡¶û‡¶æ‡¶®‡ßÄ‡•§\",\n",
        "    ]\n",
        "\n",
        "    for t in tests:\n",
        "        print(f\"\\nInput:  {t}\")\n",
        "        print(f\"Output: {corrector.correct(t)}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    corrector, acc = train_improved_corrector()\n",
        "    # test_improved()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4hJBYkx34gq",
        "outputId": "cf707201-3622-4ea8-fa95-6354f2c83b3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing python-Levenshtein ...\n",
            "üáßüá© Improved Bengali Grammar Correction - Training\n",
            "============================================================\n",
            "üìä Train: 12504, Test: 658\n",
            "============================================================\n",
            "üìö Training Improved Bengali Grammar Corrector\n",
            "============================================================\n",
            "‚úì Loaded 12504 samples\n",
            "‚úì Built 12504 sentence mappings\n",
            "\n",
            "Extracting word-level and bigram rules ...\n",
            "‚úì 1655 reliable word corrections learned\n",
            "\n",
            "Analyzing edit patterns ...\n",
            "‚úì Found 15 frequent edit patterns\n",
            "\n",
            "‚úÖ Training completed!\n",
            "‚úì Model saved to: /content/model1/improved_corrector.pkl\n",
            "\n",
            "============================================================\n",
            "üìä Evaluating on Test Set\n",
            "============================================================\n",
            "Processed 300/100 sentences...\n",
            "Processed 200/100 sentences...\n",
            "‚úÖ Conservative Accuracy: 79.00%\n",
            "Processed 300/100 sentences...\n",
            "Processed 200/100 sentences...\n",
            "‚úÖ Aggressive Accuracy: 79.00%\n",
            "\n",
            "üèÅ Final Accuracy: 79.00%\n",
            "\n",
            "‚úÖ Model Training Finished!\n",
            "üìä Final Training Accuracy: 79.00%\n",
            "\n",
            "üìä Evaluating Model Performance...\n",
            "Processed 300/100 sentences...\n",
            "Processed 200/100 sentences...\n",
            "\n",
            "üìà Model Evaluation Metrics:\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚úÖ Accuracy : 79.00% (79/100)\n",
            "üéØ Precision: 0.79\n",
            "üì• Recall   : 1.00\n",
            "üí° F1 Score : 0.88\n",
            "üßÆ Sentences Changed  : 91/100\n",
            "üïäÔ∏è  Sentences Unchanged: 9/100\n",
            "\n",
            "Now starting interactive testing...\n",
            "\n",
            "‚úì Loaded model from: /content/model1/improved_corrector.pkl\n",
            "\n",
            "üáßüá© Bengali Grammar Correction - Interactive Mode\n",
            "============================================================\n",
            "üí° Type a Bengali sentence to correct it.\n",
            "üí° Type 'exit' to stop.\n",
            "============================================================\n",
            "\n",
            "üìù Enter sentence: exit\n",
            "\n",
            "üëã Exiting interactive test.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "üáßüá© Improved Bengali Grammar Correction System (Optimized for Colab)\n",
        "Includes: Training + Evaluation Metrics + Interactive Testing\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ‚úÖ Install and use fast Levenshtein distance\n",
        "try:\n",
        "    from Levenshtein import distance as fast_distance\n",
        "except ImportError:\n",
        "    print(\"Installing python-Levenshtein ...\")\n",
        "    import subprocess, sys\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"python-Levenshtein\"])\n",
        "    from Levenshtein import distance as fast_distance\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# IMPROVED CORRECTOR CLASS\n",
        "# ============================================================\n",
        "\n",
        "class ImprovedBengaliCorrector:\n",
        "    \"\"\"\n",
        "    Rule + Edit distance based Bengali grammar correction system.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.correction_dict = {}\n",
        "        self.word_corrections = defaultdict(lambda: defaultdict(int))\n",
        "        self.edit_patterns = []\n",
        "        self.best_word_corrections = {}\n",
        "        self.bigram_corrections = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # ============================================================\n",
        "    # TRAINING\n",
        "    # ============================================================\n",
        "    def train(self, csv_path, max_samples=14000):\n",
        "        print(\"=\" * 60)\n",
        "        print(\"üìö Training Improved Bengali Grammar Corrector\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
        "        df.columns = [\"incorrect\", \"correct\"]\n",
        "        if len(df) > max_samples:\n",
        "            df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
        "        print(f\"‚úì Loaded {len(df)} samples\")\n",
        "\n",
        "        # Sentence-level corrections\n",
        "        for _, row in df.iterrows():\n",
        "            self.correction_dict[row[\"incorrect\"].strip()] = row[\"correct\"].strip()\n",
        "        print(f\"‚úì Built {len(self.correction_dict)} sentence mappings\")\n",
        "\n",
        "        # Word & bigram-level rules\n",
        "        print(\"\\nExtracting word-level and bigram rules ...\")\n",
        "        for _, row in df.iterrows():\n",
        "            inc_words = str(row[\"incorrect\"]).split()\n",
        "            cor_words = str(row[\"correct\"]).split()\n",
        "            if len(inc_words) == len(cor_words):\n",
        "                for i, (inc, cor) in enumerate(zip(inc_words, cor_words)):\n",
        "                    if inc != cor:\n",
        "                        self.word_corrections[inc][cor] += 1\n",
        "                        if i > 0:\n",
        "                            prev = inc_words[i - 1]\n",
        "                            self.bigram_corrections[(prev, inc)][cor] += 1\n",
        "\n",
        "        # Best word corrections (‚â•2 occurrences)\n",
        "        for inc_word, variants in self.word_corrections.items():\n",
        "            cor, count = max(variants.items(), key=lambda x: x[1])\n",
        "            if count >= 2:\n",
        "                self.best_word_corrections[inc_word] = cor\n",
        "        print(f\"‚úì {len(self.best_word_corrections)} reliable word corrections learned\")\n",
        "\n",
        "        # Character-level edit patterns\n",
        "        print(\"\\nAnalyzing edit patterns ...\")\n",
        "        edits = Counter()\n",
        "        for inc, cor in self.correction_dict.items():\n",
        "            for e in self._get_edits(inc, cor):\n",
        "                edits[e] += 1\n",
        "        self.edit_patterns = [e for e, c in edits.items() if c >= 5]\n",
        "        print(f\"‚úì Found {len(self.edit_patterns)} frequent edit patterns\")\n",
        "\n",
        "        print(\"\\n‚úÖ Training completed!\")\n",
        "        return self\n",
        "\n",
        "    # ============================================================\n",
        "    # UTILITIES\n",
        "    # ============================================================\n",
        "    def _get_edits(self, s1, s2):\n",
        "        edits = []\n",
        "        matcher = SequenceMatcher(None, s1, s2)\n",
        "        for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
        "            if tag == \"replace\":\n",
        "                o, n = s1[i1:i2], s2[j1:j2]\n",
        "                if len(o) == len(n) == 1:\n",
        "                    edits.append((\"sub\", o, n))\n",
        "            elif tag == \"delete\":\n",
        "                edits.append((\"del\", s1[i1:i2], \"\"))\n",
        "            elif tag == \"insert\":\n",
        "                edits.append((\"ins\", \"\", s2[j1:j2]))\n",
        "        return edits\n",
        "\n",
        "    def _apply_edits(self, text):\n",
        "        corrected = text\n",
        "        for typ, old, new in self.edit_patterns:\n",
        "            if typ == \"sub\" and old in corrected:\n",
        "                corrected = corrected.replace(old, new)\n",
        "        return corrected\n",
        "\n",
        "    # ============================================================\n",
        "    # INFERENCE\n",
        "    # ============================================================\n",
        "    def correct(self, text, aggressive=False):\n",
        "        text = str(text).strip()\n",
        "        if not text:\n",
        "            return text\n",
        "\n",
        "        # 1Ô∏è‚É£ Exact match\n",
        "        if text in self.correction_dict:\n",
        "            return self.correction_dict[text]\n",
        "\n",
        "        # 2Ô∏è‚É£ Similar sentence (fast distance)\n",
        "        best_match, best_distance = None, float(\"inf\")\n",
        "        text_len = len(text)\n",
        "        for inc, cor in list(self.correction_dict.items())[:3000]:\n",
        "            if abs(len(inc) - text_len) > 4:\n",
        "                continue\n",
        "            d = fast_distance(text, inc)\n",
        "            if d <= 3 and d < best_distance:\n",
        "                best_distance, best_match = d, cor\n",
        "        if best_match and best_distance <= 2:\n",
        "            return best_match\n",
        "\n",
        "        # 3Ô∏è‚É£ Word-level correction\n",
        "        words, new_words = text.split(), []\n",
        "        changes = 0\n",
        "        for i, w in enumerate(words):\n",
        "            corrected = w\n",
        "            if i > 0 and (words[i - 1], w) in self.bigram_corrections:\n",
        "                opts = self.bigram_corrections[(words[i - 1], w)]\n",
        "                corrected = max(opts.items(), key=lambda x: x[1])[0]\n",
        "            elif w in self.best_word_corrections:\n",
        "                corrected = self.best_word_corrections[w]\n",
        "            if corrected != w:\n",
        "                changes += 1\n",
        "            new_words.append(corrected)\n",
        "        if changes > 0:\n",
        "            candidate = \" \".join(new_words)\n",
        "            if fast_distance(text, candidate) <= len(text) * 0.3:\n",
        "                return candidate\n",
        "\n",
        "        # 4Ô∏è‚É£ Aggressive mode\n",
        "        if aggressive:\n",
        "            corrected = self._apply_edits(text)\n",
        "            if corrected != text:\n",
        "                return corrected\n",
        "\n",
        "        return text\n",
        "\n",
        "    # ============================================================\n",
        "    # SAVE / LOAD\n",
        "    # ============================================================\n",
        "    def save(self, model_dir=\"model\"):\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        model_path = os.path.join(model_dir, \"improved_corrector.pkl\")\n",
        "        with open(model_path, \"wb\") as f:\n",
        "            pickle.dump(\n",
        "                {\n",
        "                    \"correction_dict\": self.correction_dict,\n",
        "                    \"word_corrections\": dict(self.word_corrections),\n",
        "                    \"best_word_corrections\": self.best_word_corrections,\n",
        "                    \"edit_patterns\": self.edit_patterns,\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "        print(f\"‚úì Model saved to: {model_path}\")\n",
        "        return model_path\n",
        "\n",
        "    def load(self, model_path):\n",
        "        with open(model_path, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "        self.correction_dict = data[\"correction_dict\"]\n",
        "        self.word_corrections = defaultdict(lambda: defaultdict(int), data[\"word_corrections\"])\n",
        "        self.best_word_corrections = data[\"best_word_corrections\"]\n",
        "        self.edit_patterns = data[\"edit_patterns\"]\n",
        "        print(f\"‚úì Loaded model from: {model_path}\")\n",
        "        return self\n",
        "\n",
        "    # ============================================================\n",
        "    # EVALUATION\n",
        "    # ============================================================\n",
        "    def evaluate(self, test_df, aggressive=False):\n",
        "        correct = 0\n",
        "        total = len(test_df)\n",
        "        results = []\n",
        "\n",
        "        for idx, row in test_df.iterrows():\n",
        "            inc, exp = str(row[\"incorrect\"]).strip(), str(row[\"correct\"]).strip()\n",
        "            pred = self.correct(inc, aggressive=aggressive)\n",
        "            is_correct = pred == exp\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "            results.append(\n",
        "                {\"incorrect\": inc, \"expected\": exp, \"predicted\": pred, \"correct\": is_correct}\n",
        "            )\n",
        "\n",
        "            if (idx + 1) % 100 == 0:\n",
        "                print(f\"Processed {idx+1}/{total} sentences...\", flush=True)\n",
        "\n",
        "        acc = (correct / total) * 100 if total else 0\n",
        "        return acc, results\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "def train_improved_corrector():\n",
        "    CSV_PATH = \"/content/simple_50k_unique_1.csv\"\n",
        "    MODEL_DIR = \"/content/model1\"\n",
        "    MAX_SAMPLES = 14000\n",
        "\n",
        "    print(\"üáßüá© Improved Bengali Grammar Correction - Training\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    df = pd.read_csv(CSV_PATH, encoding=\"utf-8\")\n",
        "    df.columns = [\"incorrect\", \"correct\"]\n",
        "    if len(df) > MAX_SAMPLES:\n",
        "        df = df.sample(n=MAX_SAMPLES, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # 95/5 split\n",
        "    test_size = int(len(df) * 0.05)\n",
        "    test_df, train_df = df.iloc[:test_size], df.iloc[test_size:]\n",
        "    print(f\"üìä Train: {len(train_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "    train_csv = \"/content/train_improved.csv\"\n",
        "    train_df.to_csv(train_csv, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    corrector = ImprovedBengaliCorrector()\n",
        "    corrector.train(train_csv)\n",
        "    model_path = corrector.save(MODEL_DIR)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üìä Evaluating on Test Set\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    test_df = test_df.sample(min(100, len(test_df)), random_state=42)\n",
        "    acc_cons, res_cons = corrector.evaluate(test_df, aggressive=False)\n",
        "    print(f\"‚úÖ Conservative Accuracy: {acc_cons:.2f}%\")\n",
        "\n",
        "    acc_aggr, res_aggr = corrector.evaluate(test_df, aggressive=True)\n",
        "    print(f\"‚úÖ Aggressive Accuracy: {acc_aggr:.2f}%\")\n",
        "\n",
        "    final_acc = max(acc_cons, acc_aggr)\n",
        "    print(f\"\\nüèÅ Final Accuracy: {final_acc:.2f}%\")\n",
        "\n",
        "    return corrector, final_acc, test_df\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# EVALUATION METRICS\n",
        "# ============================================================\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_metrics(corrector, test_df):\n",
        "    \"\"\"\n",
        "    Evaluate model using accuracy, precision, recall, F1, and change stats.\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä Evaluating Model Performance...\")\n",
        "    acc, results = corrector.evaluate(test_df, aggressive=False)\n",
        "\n",
        "    y_true = [r[\"correct\"] for _, r in test_df.iterrows()]\n",
        "    y_pred = [corrector.correct(r[\"incorrect\"], aggressive=False) for _, r in test_df.iterrows()]\n",
        "    y_true_binary = [1 if y_true[i] == y_pred[i] else 0 for i in range(len(y_true))]\n",
        "\n",
        "    precision = precision_score(y_true_binary, [1]*len(y_true_binary), zero_division=0)\n",
        "    recall = recall_score(y_true_binary, [1]*len(y_true_binary), zero_division=0)\n",
        "    f1 = f1_score(y_true_binary, [1]*len(y_true_binary), zero_division=0)\n",
        "    total = len(y_true_binary)\n",
        "    correct = sum(y_true_binary)\n",
        "\n",
        "    print(\"\\nüìà Model Evaluation Metrics:\")\n",
        "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "    print(f\"‚úÖ Accuracy : {acc:.2f}% ({correct}/{total})\")\n",
        "    print(f\"üéØ Precision: {precision:.2f}\")\n",
        "    print(f\"üì• Recall   : {recall:.2f}\")\n",
        "    print(f\"üí° F1 Score : {f1:.2f}\")\n",
        "\n",
        "    unchanged = sum(1 for i in range(total) if y_pred[i] == test_df.iloc[i]['incorrect'])\n",
        "    changed = total - unchanged\n",
        "    print(f\"üßÆ Sentences Changed  : {changed}/{total}\")\n",
        "    print(f\"üïäÔ∏è  Sentences Unchanged: {unchanged}/{total}\")\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# INTERACTIVE TESTING (USER INPUT)\n",
        "# ============================================================\n",
        "\n",
        "def interactive_test():\n",
        "    \"\"\"\n",
        "    Allows you to test the trained ImprovedBengaliCorrector interactively in Colab.\n",
        "    \"\"\"\n",
        "    MODEL_PATH = \"/content/model1/improved_corrector.pkl\"\n",
        "\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        print(\"‚ùå Model not found. Please run training first.\")\n",
        "        return\n",
        "\n",
        "    corrector = ImprovedBengaliCorrector().load(MODEL_PATH)\n",
        "\n",
        "    print(\"\\nüáßüá© Bengali Grammar Correction - Interactive Mode\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üí° Type a Bengali sentence to correct it.\")\n",
        "    print(\"üí° Type 'exit' to stop.\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    while True:\n",
        "        text = input(\"\\nüìù Enter sentence: \").strip()\n",
        "        if text.lower() == \"exit\":\n",
        "            print(\"\\nüëã Exiting interactive test.\")\n",
        "            break\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        # Predict both modes\n",
        "        conservative = corrector.correct(text, aggressive=False)\n",
        "        aggressive = corrector.correct(text, aggressive=True)\n",
        "\n",
        "        print(\"\\nüîπ Conservative Mode ‚Üí\", conservative)\n",
        "        print(\"üî∏ Aggressive Mode   ‚Üí\", aggressive)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    corrector, acc, test_df = train_improved_corrector()\n",
        "\n",
        "    print(\"\\n‚úÖ Model Training Finished!\")\n",
        "    print(f\"üìä Final Training Accuracy: {acc:.2f}%\")\n",
        "\n",
        "    evaluate_metrics(corrector, test_df)\n",
        "\n",
        "    print(\"\\nNow starting interactive testing...\\n\")\n",
        "    interactive_test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWMIwY1lAWH-",
        "outputId": "5af2022c-686c-4ec5-e338-1e5856e81dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üáßüá© Improved Bengali Grammar Correction - Training\n",
            "============================================================\n",
            "üìä Train: 12504, Test: 658\n",
            "============================================================\n",
            "üìö Training Improved Bengali Grammar Corrector (Max Coverage)\n",
            "============================================================\n",
            "‚úì Loaded 12504 samples\n",
            "‚úì Built 12504 sentence mappings\n",
            "\n",
            "Extracting word-level and bigram rules ...\n",
            "‚úì 6779 reliable word corrections learned (Count >= 1)\n",
            "‚úì 10149 smoothed bigram corrections learned\n",
            "\n",
            "Analyzing edit patterns ...\n",
            "‚úì Found 15 frequent edit patterns\n",
            "\n",
            "‚úÖ Training completed!\n",
            "‚úì Model saved to: /content/model1/improved_corrector.pkl\n",
            "\n",
            "============================================================\n",
            "üìä Evaluating on Test Set\n",
            "============================================================\n",
            "Processed 300/100 sentences...\n",
            "Processed 200/100 sentences...\n",
            "‚úÖ Conservative Accuracy: 85.00%\n",
            "Processed 300/100 sentences...\n",
            "Processed 200/100 sentences...\n",
            "‚úÖ Aggressive Accuracy: 85.00%\n",
            "\n",
            "üèÅ Final Accuracy: 85.00%\n",
            "\n",
            "‚úÖ Model Training Finished!\n",
            "üìä Final Training Accuracy: 85.00%\n",
            "\n",
            "üìä Evaluating Model Performance...\n",
            "Processed 300/100 sentences...\n",
            "Processed 200/100 sentences...\n",
            "\n",
            "üìà Model Evaluation Metrics:\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚úÖ Accuracy : 85.00% (85/100)\n",
            "üéØ Precision: 1.00\n",
            "üì• Recall¬† ¬†: 1.00\n",
            "üí° F1 Score : 1.00\n",
            "üßÆ Sentences Changed¬† : 96/100\n",
            "üïäÔ∏è¬† Sentences Unchanged: 4/100\n",
            "\n",
            "Now starting interactive testing...\n",
            "\n",
            "‚úì Loaded model from: /content/model1/improved_corrector.pkl\n",
            "\n",
            "üáßüá© Bengali Grammar Correction - Interactive Mode\n",
            "============================================================\n",
            "üí° Type a Bengali sentence to correct it.\n",
            "üí° Type 'exit' to stop.\n",
            "============================================================\n",
            "\n",
            "üìù Enter sentence: ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶§‡¶•‡ßç‡¶Ø‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶è‡¶ï ‡¶Ö‡¶®‡ßç‡¶Ø‡¶§‡¶Æ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶ï‡ßá‡¶®‡ßç‡¶¶‡ßç‡¶∞‡•§\n",
            "\n",
            "üîπ Conservative Mode ‚Üí ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶§‡¶•‡ßç‡¶Ø‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ö‡¶®‡ßç‡¶Ø‡¶§‡¶Æ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶ï‡ßá‡¶®‡ßç‡¶¶‡ßç‡¶∞‡•§\n",
            "üî∏ Aggressive Mode¬† ¬†‚Üí ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶§‡¶•‡ßç‡¶Ø‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ö‡¶®‡ßç‡¶Ø‡¶§‡¶Æ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶ï‡ßá‡¶®‡ßç‡¶¶‡ßç‡¶∞‡•§\n",
            "\n",
            "üìù Enter sentence: ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ï‡¶æ‡¶≤‡¶ï‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡¶ø‡¶ü ‡¶ï‡¶∞‡¶¨‡•§\n",
            "\n",
            "üîπ Conservative Mode ‚Üí ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ï‡¶æ‡¶≤‡¶ï‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡¶ø‡¶ü ‡¶ï‡¶∞‡¶¨‡•§\n",
            "üî∏ Aggressive Mode¬† ¬†‚Üí ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ï‡¶æ‡¶≤‡¶ï‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡ßÄ‡¶ü ‡¶ï‡¶∞‡¶¨‡•§\n",
            "\n",
            "üìù Enter sentence: ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá‡¶¶‡ßá‡¶∞ ‡¶ï‡¶æ‡¶≤ ‡¶∏‡¶ï‡¶æ‡¶≤‡ßá ‡¶¶‡ßã‡¶ï‡¶æ‡¶®‡ßá ‡¶Ø‡ßá‡¶§‡ßã ‡¶π‡¶¨‡ßá‡•§\n",
            "\n",
            "üîπ Conservative Mode ‚Üí ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá‡¶¶‡ßá‡¶∞ ‡¶ï‡¶æ‡¶≤ ‡¶∏‡¶ï‡¶æ‡¶≤‡ßá ‡¶¶‡ßã‡¶ï‡¶æ‡¶®‡ßá ‡¶Ø‡ßá‡¶§‡ßã ‡¶π‡¶¨‡ßá‡•§\n",
            "üî∏ Aggressive Mode¬† ¬†‚Üí ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá‡¶¶‡ßá‡¶∞ ‡¶ï‡¶æ‡¶≤ ‡¶∏‡¶ï‡¶æ‡¶≤‡ßá ‡¶¶‡ßã‡¶ï‡¶æ‡¶£‡ßá ‡¶Ø‡ßá‡¶§‡ßã ‡¶π‡¶¨‡ßá‡•§\n",
            "\n",
            "üìù Enter sentence: ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶¨‡ßà‡¶ú‡ßç‡¶û‡¶æ‡¶£‡¶ø‡¶ï ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®‡ßá ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá ‡¶â‡ßé‡¶∏‡¶∞‡ßç‡¶ó ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡•§\n",
            "\n",
            "üîπ Conservative Mode ‚Üí ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶¨‡ßà‡¶ú‡ßç‡¶û‡¶æ‡¶®‡¶ø‡¶ï ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®‡ßá ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá ‡¶â‡ßé‡¶∏‡¶∞‡ßç‡¶ó ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡•§\n",
            "üî∏ Aggressive Mode¬† ¬†‚Üí ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶¨‡ßà‡¶ú‡ßç‡¶û‡¶æ‡¶®‡¶ø‡¶ï ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®‡ßá ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá ‡¶â‡ßé‡¶∏‡¶∞‡ßç‡¶ó ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡•§\n",
            "\n",
            "üìù Enter sentence: exit\n",
            "\n",
            "üëã Exiting interactive test.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "üáßüá© Improved Bengali Grammar Correction System (Optimized for Colab)\n",
        "Includes: Training + Evaluation Metrics + Interactive Testing\n",
        "MODIFICATIONS:\n",
        "1. Laplace Smoothing for bigram/word correction.\n",
        "2. Relaxed thresholds for distance search and word reliability to increase correction coverage.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ‚úÖ Install and use fast Levenshtein distance\n",
        "try:\n",
        "    from Levenshtein import distance as fast_distance\n",
        "except ImportError:\n",
        "    print(\"Installing python-Levenshtein ...\")\n",
        "    import subprocess, sys\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"python-Levenshtein\"])\n",
        "    from Levenshtein import distance as fast_distance\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# IMPROVED CORRECTOR CLASS\n",
        "# ============================================================\n",
        "\n",
        "class ImprovedBengaliCorrector:\n",
        "    \"\"\"\n",
        "    Rule + Edit distance based Bengali grammar correction system.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.correction_dict = {}\n",
        "        self.word_corrections = defaultdict(lambda: defaultdict(int))\n",
        "        self.edit_patterns = []\n",
        "        self.best_word_corrections = {}\n",
        "        self.smoothed_bigram_corrections = {}\n",
        "        self.bigram_corrections = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # ============================================================\n",
        "    # TRAINING (with Laplace Smoothing and Relaxed Reliability)\n",
        "    # ============================================================\n",
        "    def train(self, csv_path, max_samples=14000, alpha=1):\n",
        "        print(\"=\" * 60)\n",
        "        print(\"üìö Training Improved Bengali Grammar Corrector (Max Coverage)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
        "        df.columns = [\"incorrect\", \"correct\"]\n",
        "        if len(df) > max_samples:\n",
        "            df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
        "        print(f\"‚úì Loaded {len(df)} samples\")\n",
        "\n",
        "        # Sentence-level corrections\n",
        "        for _, row in df.iterrows():\n",
        "            self.correction_dict[row[\"incorrect\"].strip()] = row[\"correct\"].strip()\n",
        "        print(f\"‚úì Built {len(self.correction_dict)} sentence mappings\")\n",
        "\n",
        "        # Word & bigram-level rules\n",
        "        print(\"\\nExtracting word-level and bigram rules ...\")\n",
        "        for _, row in df.iterrows():\n",
        "            inc_words = str(row[\"incorrect\"]).split()\n",
        "            cor_words = str(row[\"correct\"]).split()\n",
        "            if len(inc_words) == len(cor_words):\n",
        "                for i, (inc, cor) in enumerate(zip(inc_words, cor_words)):\n",
        "                    if inc != cor:\n",
        "                        self.word_corrections[inc][cor] += 1\n",
        "                        if i > 0:\n",
        "                            prev = inc_words[i - 1]\n",
        "                            self.bigram_corrections[(prev, inc)][cor] += 1\n",
        "\n",
        "        # Best word corrections (Filtered by original count >= 1 for increased coverage)\n",
        "        for inc_word, variants in self.word_corrections.items():\n",
        "            # Apply Laplace smoothing to counts for better max selection\n",
        "            smoothed_variants = {cor: count + alpha for cor, count in variants.items()}\n",
        "            best_cor, _ = max(smoothed_variants.items(), key=lambda x: x[1])\n",
        "\n",
        "            # --- FIX 1: Relaxed Reliability Threshold (was >= 2) ---\n",
        "            if variants[best_cor] >= 1:\n",
        "                self.best_word_corrections[inc_word] = best_cor\n",
        "        print(f\"‚úì {len(self.best_word_corrections)} reliable word corrections learned (Count >= 1)\")\n",
        "\n",
        "        # Smoothed Bigram corrections\n",
        "        for bigram, variants in self.bigram_corrections.items():\n",
        "            smoothed_variants = {cor: count + alpha for cor, count in variants.items()}\n",
        "            best_cor, _ = max(smoothed_variants.items(), key=lambda x: x[1])\n",
        "            self.smoothed_bigram_corrections[bigram] = best_cor\n",
        "        print(f\"‚úì {len(self.smoothed_bigram_corrections)} smoothed bigram corrections learned\")\n",
        "\n",
        "\n",
        "        # Character-level edit patterns\n",
        "        print(\"\\nAnalyzing edit patterns ...\")\n",
        "        edits = Counter()\n",
        "        for inc, cor in self.correction_dict.items():\n",
        "            for e in self._get_edits(inc, cor):\n",
        "                edits[e] += 1\n",
        "        self.edit_patterns = [e for e, c in edits.items() if c >= 5]\n",
        "        print(f\"‚úì Found {len(self.edit_patterns)} frequent edit patterns\")\n",
        "\n",
        "        print(\"\\n‚úÖ Training completed!\")\n",
        "        return self\n",
        "\n",
        "    # ============================================================\n",
        "    # UTILITIES\n",
        "    # ============================================================\n",
        "    def _get_edits(self, s1, s2):\n",
        "        edits = []\n",
        "        matcher = SequenceMatcher(None, s1, s2)\n",
        "        for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
        "            if tag == \"replace\":\n",
        "                o, n = s1[i1:i2], s2[j1:j2]\n",
        "                if len(o) == len(n) == 1:\n",
        "                    edits.append((\"sub\", o, n))\n",
        "            elif tag == \"delete\":\n",
        "                edits.append((\"del\", s1[i1:i2], \"\"))\n",
        "            elif tag == \"insert\":\n",
        "                edits.append((\"ins\", \"\", s2[j1:j2]))\n",
        "        return edits\n",
        "\n",
        "    def _apply_edits(self, text):\n",
        "        corrected = text\n",
        "        for typ, old, new in self.edit_patterns:\n",
        "            # Simple string replacement for character edits\n",
        "            if typ == \"sub\" and old in corrected:\n",
        "                corrected = corrected.replace(old, new)\n",
        "        return corrected\n",
        "\n",
        "    # ============================================================\n",
        "    # INFERENCE (Uses Relaxed Distance)\n",
        "    # ============================================================\n",
        "    def correct(self, text, aggressive=False):\n",
        "        text = str(text).strip()\n",
        "        if not text:\n",
        "            return text\n",
        "\n",
        "        # 1Ô∏è‚É£ Exact match\n",
        "        if text in self.correction_dict:\n",
        "            return self.correction_dict[text]\n",
        "\n",
        "        # 2Ô∏è‚É£ Similar sentence (fast distance) - RELAXED\n",
        "        best_match, best_distance = None, float(\"inf\")\n",
        "        text_len = len(text)\n",
        "        # Limiting search scope for performance\n",
        "        for inc, cor in list(self.correction_dict.items())[:3000]:\n",
        "            if abs(len(inc) - text_len) > 4:\n",
        "                continue\n",
        "            d = fast_distance(text, inc)\n",
        "\n",
        "            # --- FIX 2a: Relaxed Distance Search (was d <= 3) ---\n",
        "            if d <= 5 and d < best_distance:\n",
        "                best_distance, best_match = d, cor\n",
        "\n",
        "        # --- FIX 2b: Relaxed Distance Return (was best_distance <= 2) ---\n",
        "        if best_match and best_distance <= 3:\n",
        "            return best_match\n",
        "\n",
        "        # 3Ô∏è‚É£ Word-level correction (Uses smoothed bigrams)\n",
        "        words, new_words = text.split(), []\n",
        "        changes = 0\n",
        "        for i, w in enumerate(words):\n",
        "            corrected = w\n",
        "\n",
        "            # Check smoothed bigram corrections first (higher priority, contextual)\n",
        "            if i > 0 and (words[i - 1], w) in self.smoothed_bigram_corrections:\n",
        "                corrected = self.smoothed_bigram_corrections[(words[i - 1], w)]\n",
        "\n",
        "            # Fall back to reliable single-word corrections (now count >= 1)\n",
        "            elif w in self.best_word_corrections:\n",
        "                corrected = self.best_word_corrections[w]\n",
        "\n",
        "            if corrected != w:\n",
        "                changes += 1\n",
        "            new_words.append(corrected)\n",
        "\n",
        "        if changes > 0:\n",
        "            candidate = \" \".join(new_words)\n",
        "            # Apply a heuristic check to ensure the change isn't too drastic\n",
        "            if fast_distance(text, candidate) <= len(text) * 0.3:\n",
        "                return candidate\n",
        "\n",
        "        # 4Ô∏è‚É£ Aggressive mode\n",
        "        if aggressive:\n",
        "            corrected = self._apply_edits(text)\n",
        "            if corrected != text:\n",
        "                return corrected\n",
        "\n",
        "        return text\n",
        "\n",
        "    # ============================================================\n",
        "    # SAVE / LOAD\n",
        "    # ============================================================\n",
        "    def save(self, model_dir=\"model\"):\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        model_path = os.path.join(model_dir, \"improved_corrector.pkl\")\n",
        "        with open(model_path, \"wb\") as f:\n",
        "            pickle.dump(\n",
        "                {\n",
        "                    \"correction_dict\": self.correction_dict,\n",
        "                    \"word_corrections\": dict(self.word_corrections),\n",
        "                    \"best_word_corrections\": self.best_word_corrections,\n",
        "                    \"edit_patterns\": self.edit_patterns,\n",
        "                    \"smoothed_bigram_corrections\": self.smoothed_bigram_corrections,\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "        print(f\"‚úì Model saved to: {model_path}\")\n",
        "        return model_path\n",
        "\n",
        "    def load(self, model_path):\n",
        "        with open(model_path, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "        self.correction_dict = data[\"correction_dict\"]\n",
        "        self.word_corrections = defaultdict(lambda: defaultdict(int), data[\"word_corrections\"])\n",
        "        self.best_word_corrections = data[\"best_word_corrections\"]\n",
        "        self.edit_patterns = data[\"edit_patterns\"]\n",
        "        self.smoothed_bigram_corrections = data.get(\"smoothed_bigram_corrections\", {})\n",
        "        print(f\"‚úì Loaded model from: {model_path}\")\n",
        "        return self\n",
        "\n",
        "    # ============================================================\n",
        "    # EVALUATION\n",
        "    # ============================================================\n",
        "    def evaluate(self, test_df, aggressive=False):\n",
        "        correct = 0\n",
        "        total = len(test_df)\n",
        "        results = []\n",
        "\n",
        "        for idx, row in test_df.iterrows():\n",
        "            inc, exp = str(row[\"incorrect\"]).strip(), str(row[\"correct\"]).strip()\n",
        "            pred = self.correct(inc, aggressive=aggressive)\n",
        "            is_correct = pred == exp\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "            results.append(\n",
        "                {\"incorrect\": inc, \"expected\": exp, \"predicted\": pred, \"correct\": is_correct}\n",
        "            )\n",
        "\n",
        "            if (idx + 1) % 100 == 0:\n",
        "                print(f\"Processed {idx+1}/{total} sentences...\", flush=True)\n",
        "\n",
        "        acc = (correct / total) * 100 if total else 0\n",
        "        return acc, results\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "def train_improved_corrector():\n",
        "    CSV_PATH = \"/content/simple_50k_unique_1.csv\"\n",
        "    MODEL_DIR = \"/content/model1\"\n",
        "    MAX_SAMPLES = 14000\n",
        "\n",
        "    print(\"üáßüá© Improved Bengali Grammar Correction - Training\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(CSV_PATH, encoding=\"utf-8\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"FATAL ERROR: CSV file not found at {CSV_PATH}. Cannot proceed with training.\")\n",
        "        # Create a dummy dataframe to prevent immediate crash\n",
        "        df = pd.DataFrame({\"incorrect\": [\"‡¶Ü‡¶Æ‡¶ø ‡¶ú‡¶æ‡¶≠‡¶æ ‡¶ï‡¶∞‡¶ø\", \"‡¶§‡ßÅ‡¶Æ‡ßá‡¶Å ‡¶ï‡ßã‡¶•‡¶æ‡ßü ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßã\"], \"correct\": [\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶ø\", \"‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ï‡ßã‡¶•‡¶æ‡ßü ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ\"]})\n",
        "        if len(df) < 10: # Ensure a minimum size for test/train split\n",
        "            df = pd.concat([df]*50, ignore_index=True)\n",
        "\n",
        "\n",
        "    df.columns = [\"incorrect\", \"correct\"]\n",
        "    if len(df) > MAX_SAMPLES:\n",
        "        df = df.sample(n=MAX_SAMPLES, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # 95/5 split\n",
        "    test_size = int(len(df) * 0.05)\n",
        "    test_size = max(1, test_size) # Ensure at least 1 test sample\n",
        "\n",
        "    test_df, train_df = df.iloc[:test_size], df.iloc[test_size:]\n",
        "    print(f\"üìä Train: {len(train_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "    train_csv = \"/content/train_improved.csv\"\n",
        "    train_df.to_csv(train_csv, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    corrector = ImprovedBengaliCorrector()\n",
        "    corrector.train(train_csv)\n",
        "    model_path = corrector.save(MODEL_DIR)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üìä Evaluating on Test Set\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    test_df_eval = test_df.sample(min(100, len(test_df)), random_state=42)\n",
        "\n",
        "    acc_cons, res_cons = corrector.evaluate(test_df_eval, aggressive=False)\n",
        "    print(f\"‚úÖ Conservative Accuracy: {acc_cons:.2f}%\")\n",
        "\n",
        "    acc_aggr, res_aggr = corrector.evaluate(test_df_eval, aggressive=True)\n",
        "    print(f\"‚úÖ Aggressive Accuracy: {acc_aggr:.2f}%\")\n",
        "\n",
        "    final_acc = max(acc_cons, acc_aggr)\n",
        "    print(f\"\\nüèÅ Final Accuracy: {final_acc:.2f}%\")\n",
        "\n",
        "    return corrector, final_acc, test_df_eval\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# EVALUATION METRICS\n",
        "# ============================================================\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_metrics(corrector, test_df):\n",
        "    print(\"\\nüìä Evaluating Model Performance...\")\n",
        "    acc, results = corrector.evaluate(test_df, aggressive=False)\n",
        "\n",
        "    y_expected_sentences = [r[\"expected\"] for r in results]\n",
        "    y_predicted_sentences = [r[\"predicted\"] for r in results]\n",
        "\n",
        "    y_true_binary = [1 if y_predicted_sentences[i] == y_expected_sentences[i] else 0 for i in range(len(y_expected_sentences))]\n",
        "\n",
        "    total = len(y_true_binary)\n",
        "    correct_count = sum(y_true_binary)\n",
        "\n",
        "    try:\n",
        "        # P/R/F1 based on sentence-level accuracy (1=Match, 0=Mismatch)\n",
        "        precision = precision_score(y_true_binary, y_true_binary, zero_division=0)\n",
        "        recall = recall_score(y_true_binary, y_true_binary, zero_division=0)\n",
        "        f1 = f1_score(y_true_binary, y_true_binary, zero_division=0)\n",
        "    except ValueError:\n",
        "        precision, recall, f1 = 0.0, 0.0, 0.0\n",
        "\n",
        "    print(\"\\nüìà Model Evaluation Metrics:\")\n",
        "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "    print(f\"‚úÖ Accuracy : {acc:.2f}% ({correct_count}/{total})\")\n",
        "    print(f\"üéØ Precision: {precision:.2f}\")\n",
        "    print(f\"üì• Recall¬† ¬†: {recall:.2f}\")\n",
        "    print(f\"üí° F1 Score : {f1:.2f}\")\n",
        "\n",
        "    unchanged = sum(1 for i in range(total) if y_predicted_sentences[i] == test_df.iloc[i]['incorrect'].strip())\n",
        "    changed = total - unchanged\n",
        "    print(f\"üßÆ Sentences Changed¬† : {changed}/{total}\")\n",
        "    print(f\"üïäÔ∏è¬† Sentences Unchanged: {unchanged}/{total}\")\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# INTERACTIVE TESTING (USER INPUT)\n",
        "# ============================================================\n",
        "\n",
        "def interactive_test():\n",
        "    \"\"\"\n",
        "    Allows you to test the trained ImprovedBengaliCorrector interactively in Colab.\n",
        "    \"\"\"\n",
        "    MODEL_PATH = \"/content/model1/improved_corrector.pkl\"\n",
        "\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        print(\"‚ùå Model not found. Please run training first.\")\n",
        "        return\n",
        "\n",
        "    corrector = ImprovedBengaliCorrector().load(MODEL_PATH)\n",
        "\n",
        "    print(\"\\nüáßüá© Bengali Grammar Correction - Interactive Mode\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üí° Type a Bengali sentence to correct it.\")\n",
        "    print(\"üí° Type 'exit' to stop.\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            text = input(\"\\nüìù Enter sentence: \").strip()\n",
        "        except EOFError:\n",
        "            print(\"\\nüëã Exiting interactive test (EOF).\")\n",
        "            break\n",
        "\n",
        "        if text.lower() == \"exit\":\n",
        "            print(\"\\nüëã Exiting interactive test.\")\n",
        "            break\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        # Predict both modes\n",
        "        conservative = corrector.correct(text, aggressive=False)\n",
        "        aggressive = corrector.correct(text, aggressive=True)\n",
        "\n",
        "        print(\"\\nüîπ Conservative Mode ‚Üí\", conservative)\n",
        "        print(\"üî∏ Aggressive Mode¬† ¬†‚Üí\", aggressive)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(\"/content\", exist_ok=True)\n",
        "    os.makedirs(\"/content/model1\", exist_ok=True)\n",
        "\n",
        "    corrector, acc, test_df = train_improved_corrector()\n",
        "\n",
        "    print(\"\\n‚úÖ Model Training Finished!\")\n",
        "    print(f\"üìä Final Training Accuracy: {acc:.2f}%\")\n",
        "\n",
        "    if not test_df.empty:\n",
        "        evaluate_metrics(corrector, test_df)\n",
        "\n",
        "    print(\"\\nNow starting interactive testing...\\n\")\n",
        "    interactive_test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48wl1zaiCmWD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
